

# 5D YOLOv8 + GPS â€” Multi-Modal Object Detection and Localisation

A PyTorch implementation of Ultralytics **YOLOv8** extended to consume  
**RGB + Depth + Thermal** (5-channel) input and regress a global **GPS (lat, lon)**  
for every frame.

[![Python 3.8+](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)  
[![PyTorch 2.1+](https://img.shields.io/badge/PyTorch-2.1%2B-red.svg)](https://pytorch.org/)  
[![Ultralytics 8.3.x](https://img.shields.io/badge/Ultralytics-YOLOv8-8.3.x-green.svg)](https://github.com/ultralytics/ultralytics)  
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)



## ğŸ“‹ Overview

* **5-channel input**â€ƒ`RGB (3) + Depth (1) + Thermal (1)`  
* **Thermal fusion**â€ƒInjected at YOLOâ€™s mid-feature via a forward hook  
* **Dual-head output**â€ƒStandard YOLO detections **plus** a 2-D GPS vector  
* **Joint training**â€ƒUltralytics v8 detection loss + MSE GPS loss â€” all weights train

Ideal for robotics, autonomous driving, or any scenario where object
detection and coarse localisation must be learned from multiple sensors.


## ğŸ›  Installation

```bash
git clone https://github.com/farshidrayhancv/yolo5D_plus_gps.git
cd yolo5D_plus_gps

# core dependencies
pip install torch torchvision matplotlib tqdm pillow ultralytics==8.3.140
---

> **Requirements**  Python 3.8+, PyTorch 2.1+, torchvision, Ultralytics 8.3.x



## ğŸ’» Usage

### 1 Â· Training

```bash
python train_5d.py          # trains on VOC 2012 with synthetic depth+thermal
```

### 2 Â· Inference (post-processed boxes + GPS)

```python
import torch
from train_5d import YOLO5D          # model class lives in the training script

model = YOLO5D().eval()
model.load_state_dict(torch.load("ckpts/yolo5d_best.pt", map_location="cpu"))

rgb      = torch.rand(3, 320, 320)
depth    = torch.rand(1, 320, 320)
thermal  = torch.rand(1, 96, 96)
rgbd     = torch.cat([rgb, depth]).unsqueeze(0)

results, gps = model.predict(rgbd, thermal)   # NMS boxes + (1,2) GPS

print("boxes:", results[0].boxes.xyxy)
print("GPS  :", gps.squeeze().tolist())
```

---

## ğŸ§  Architecture

```mermaid
flowchart LR
    %% Inputs
    RGB["RGB<br>(3Ã—HÃ—W)"]
    DEPTH["Depth<br>(1Ã—HÃ—W)"]
    THERM["Thermal<br>(1Ã—96Ã—96)"]

    RGB ---|"concat"| DEPTH
    RGBD[/"RGB-D<br>(4Ã—HÃ—W)"/]

    subgraph Front-End
        ADAPT["RGBD2RGB<br>1Ã—1 conv"]:::block
    end

    subgraph YOLOv8 Backbone + Neck
        Y0["layers 0-6"]:::block
        FUSE["(+ 0.1 Â· thermal)<br><i>hook</i>"]:::fuse
        Y1["layers 7-end"]:::block
    end

    subgraph Heads
        DET["Detection<br>head"]:::head
        GPS["GPS MLP"]:::head
    end

    RGBD  --> ADAPT --> Y0
    THERM -->|Thermal-CNN| TPROC["thermal<br>feat"]:::block
    TPROC -.->|hook| FUSE
    Y0 --> FUSE --> Y1
    Y1 --> DET
    Y1 --> GPS

    classDef block fill:#f6f8fa,stroke:#333;
    classDef head  fill:#d5f5e3,stroke:#1e8449;
    classDef fuse  fill:#fff4e5,stroke:#e67e22,stroke-dasharray:5 3;
```

* **RGB-D Adapter** â€“ collapses 4 â†’ 3 channels so YOLO can ingest the frame.
* **Thermal Processor** â€“ embeds & upsamples the thermal map.
* **Hook** â€“ adds thermal features into layer-6 activations *during* forward pass.
* **Heads** â€“ YOLO detection head predicts boxes/classes; a small MLP regresses GPS.

---

## ğŸ“Š Current Demo Numbers (synthetic)

| Model     | Input | Dataset    | DetÂ loss â†“ | GPSÂ loss â†“ |
| --------- | ----- | ---------- | ---------- | ---------- |
| 5D-YOLO-n | 320   | VOC 2012\* | 7.7        | 1.8 e-5    |

\* Depth / thermal and GPS are synthetic placeholders â€” detection loss is
real but GPS numbers are meaningless until real coordinates are supplied.

---

## ğŸ—‚ Project Structure

```
â”œâ”€â”€ train_5d.py               # full training + model definition
â”œâ”€â”€ ckpts/                    # saved checkpoints
â”œâ”€â”€ data/                     # VOC dataset will download here
â””â”€â”€ README.md
```

---

## ğŸ”„ Customisation Tips

* **Plug in real depth & thermal**
  Replace the random tensors in `VOCExtended.__getitem__`.

* **Use real GPS labels**
  Swap the fixed `[0.5,0.5]` with your `(lat_norm, lon_norm)` values.

* **Tune loss balance**
  Adjust `LAMBDA_GPS` to weight GPS vs. detection learning.

---

## ğŸ“‹ TODO

* [ ] Integrate real multi-sensor datasets (NYU Depth, FLIR, etc.)
* [ ] Hyper-parameter sweep for GPS/det loss weighting
* [ ] Export ONNX / TensorRT for edge deployment
* [ ] Benchmark on Jetson & Raspberry Pi

---

## ğŸ“„ License

Released under the MIT License.

---

## ğŸ™ Acknowledgements

* **[Ultralytics YOLOv8](https://github.com/ultralytics/ultralytics)** â€“ fantastic open-source detector.
* Pascal VOC for the benchmark images.


